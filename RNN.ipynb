{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load datasets\n",
    "df_children_books_final_encoded = pd.read_csv('df_children_books_final.csv')\n",
    "df_interactions_train = pd.read_csv('df_interactions_train.csv')\n",
    "df_interactions_val = pd.read_csv('df_interactions_val.csv')\n",
    "df_interactions_test = pd.read_csv('test_interactions.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Preprocess Data\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "# Encode user and item ids\n",
    "df_interactions_train['user_id'] = user_encoder.fit_transform(df_interactions_train['user_id'])\n",
    "df_interactions_train['item_id'] = item_encoder.fit_transform(df_interactions_train['item_id'])\n",
    "df_interactions_val['user_id'] = user_encoder.transform(df_interactions_val['user_id'])\n",
    "df_interactions_val['item_id'] = item_encoder.transform(df_interactions_val['item_id'])\n",
    "\n",
    "# Sort interactions by timestamp if available to maintain sequential data\n",
    "\n",
    "df_interactions_train = df_interactions_train.sort_values(by=['user_id', 'review_age'])\n",
    "df_interactions_val = df_interactions_val.sort_values(by=['user_id', 'review_age'])\n",
    "\n",
    "# Group interactions by user to create sequences\n",
    "user_item_sequences = df_interactions_train.groupby('user_id')['item_id'].apply(list)\n",
    "val_user_item_sequences = df_interactions_val.groupby('user_id')['item_id'].apply(list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Dataset Class for RNN input\n",
    "class InteractionDataset(Dataset):\n",
    "    def __init__(self, user_sequences, sequence_length=10):\n",
    "        self.user_sequences = user_sequences\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.user_sequences[idx]\n",
    "        sequence = sequence[-self.sequence_length:]  # Get last N items for each sequence\n",
    "        x = torch.tensor(sequence[:-1], dtype=torch.long)  # All except the last item\n",
    "        y = torch.tensor(sequence[1:], dtype=torch.long)   # All except the first item\n",
    "        return x, y\n",
    "\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, user_sequences, sequence_length=10):\n",
    "        self.user_sequences = user_sequences\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.user_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.user_sequences[idx]\n",
    "        sequence = sequence[-self.sequence_length:]  # Get last N items\n",
    "        x = torch.tensor(sequence[:-1], dtype=torch.long)  # Input sequence\n",
    "        y = torch.tensor(sequence[-1], dtype=torch.long)   # Next item to predict\n",
    "        return x, y\n",
    "    \n",
    "# Prepare DataLoader\n",
    "sequence_length = 10\n",
    "dataset = InteractionDataset(list(user_item_sequences), sequence_length=sequence_length)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the RNN Model\n",
    "class RNNRecommendationModel(nn.Module):\n",
    "    def __init__(self, num_items, embedding_dim=50, hidden_dim=100):\n",
    "        super(RNNRecommendationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_items)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Embed item indices\n",
    "        rnn_out, _ = self.rnn(x)  # RNN output\n",
    "        logits = self.fc(rnn_out)  # Fully connected layer to output predictions\n",
    "        return logits\n",
    "\n",
    "# Initialize Model, Loss, and Optimizer\n",
    "num_items = len(item_encoder.classes_)\n",
    "model = RNNRecommendationModel(num_items)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 4: Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        # Reshape for loss calculation\n",
    "        loss = criterion(output.view(-1, num_items), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = ValidationDataset(list(val_user_item_sequences), sequence_length=sequence_length)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model(model, dataloader, top_k=10):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    top_k_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            output = model(x)\n",
    "            # Get the last time step's predictions (last item in sequence prediction)\n",
    "            preds = output[:, -1, :]  # Shape: (batch_size, num_items)\n",
    "            \n",
    "            # Calculate top-1 accuracy\n",
    "            _, top1_preds = torch.max(preds, 1)\n",
    "            correct += (top1_preds == y).sum().item()\n",
    "            \n",
    "            # Calculate top-k accuracy\n",
    "            topk_preds = torch.topk(preds, k=top_k, dim=1)[1]\n",
    "            top_k_correct += sum([y[i].item() in topk_preds[i] for i in range(len(y))])\n",
    "\n",
    "            total += y.size(0)\n",
    "    \n",
    "    top1_accuracy = correct / total\n",
    "    topk_accuracy = top_k_correct / total\n",
    "    print(f\"Top-1 Accuracy: {top1_accuracy:.4f}\")\n",
    "    print(f\"Top-{top_k} Accuracy: {topk_accuracy:.4f}\")\n",
    "\n",
    "# Step 6: Evaluate the model on the validation set\n",
    "evaluate_model(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
